========================================JD===========================================
Experience	Skills
‚Ä¢ Designed and implemented CI/CD pipelines for containerized applications using Jenkins, GitLab CI/CD, or CircleCI, enabling continuous delivery.

‚Ä¢ Worked closely with development teams to troubleshoot issues, optimize application performance, and ensure compatibility with Kubernetes environments.

‚Ä¢ Conducted regular audits and assessments to identify and remediate security vulnerabilities, ensuring compliance with industry standards.

‚Ä¢ Documented deployment procedures, configuration settings, and troubleshooting steps to facilitate knowledge sharing and collaboration among team members.

‚Ä¢ Managed and maintained Linux/Unix servers and virtualization platforms such as VMware or KVM.

‚Ä¢ Installed, configured, and maintained container runtimes such as Docker or containerd.

‚Ä¢ Automated infrastructure provisioning and configuration using tools like Ansible, Puppet, or Chef.

‚Ä¢ Implemented backup and disaster recovery solutions to ensure data integrity and minimize downtime.

‚Ä¢ Provided technical support and troubleshooting for server and network infrastructure issues.

‚Ä¢ Deployed, configured, and managed Kubernetes clusters on-premises or on cloud platforms such as AWS, Azure, or GCP.

‚Ä¢ Implemented best practices for cluster security, including role-based access control (RBAC), network policies, and encryption at rest and in transit.

‚Ä¢ Automated cluster provisioning and management tasks using tools like Terraform, Ansible, or Helm charts, reducing manual effort by 40%.

‚Ä¢ Monitored cluster health and performance metrics using Prometheus and Grafana, ensuring high availability and performance.	‚Ä¢ Proficient in Kubernetes deployment, configuration, and administration.

‚Ä¢ Strong understanding of containerization technologies (Docker, containerd).

‚Ä¢ Experience with cloud computing platforms (AWS, Azure, GCP).

‚Ä¢ Knowledge of infrastructure as code (Terraform, Ansible).

‚Ä¢ Familiarity with CI/CD pipelines and automation tools (Jenkins, GitLab CI/CD).

‚Ä¢ Excellent troubleshooting and problem-solving skills.

‚Ä¢ Strong communication and collaboration abilities.

************************************************************************************************************
============================================================================================================


1. You‚Äôre managing an on-premises Kubernetes cluster, and developers report that:

‚ÄúWhen we create new pods, they stay in Pending for several minutes, even though nodes are healthy and have enough resources.‚Äù

You check the nodes ‚Äî all Ready.
You also check the pods ‚Äî they‚Äôre waiting for scheduling.

You suspect something is wrong with the control plane.

üëâ Question:
What components of the control plane would you investigate, in what order, and what would you specifically look for in each component to identify the cause of the scheduling delay?

Answer 

If new pods remain Pending while nodes are healthy, I‚Äôd first check the scheduler ‚Äî it‚Äôs responsible for assigning pods to nodes. I‚Äôd look at the scheduler logs for ‚ÄòFailedScheduling‚Äô messages.

If the scheduler looks fine, I‚Äôd check the controller-manager to see if pods are even being created correctly.

Then I‚Äôd look at the API server logs and metrics to check for latency ‚Äî slow etcd or API server overload can delay scheduling. Finally, 
I‚Äôd verify etcd health using etcdctl endpoint health to ensure low latency and no timeouts. All these components are tightly coupled in scheduling flow.‚Äù
==================================================================================================================

2. ‚öôÔ∏è Question 15 (Node Metadata & Scheduling Logic)

You have three nodes:

node1 labeled as env=prod and annotated with team=backend

node2 labeled as env=dev and annotated with team=frontend

node3 labeled as env=stage and annotated with team=qa

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: team
          operator: In
          values:
          - backend

Where will this pod get scheduled ‚Äî and why?

Answer: 

Yes ‚Äî the pod will be scheduled on node1 because:

The key team=backend exists as an annotation on node1.

However‚Ä¶ üß© nodeAffinity only works with labels, not annotations.

So actually ‚Äî in this exact YAML, the pod will stay in a Pending state unless that team=backend is a label (not an annotation).
===================================================================================================================

3. üî• Question ‚Äî ETCD Disaster Scenario (On-Prem Only)

Your on-prem cluster suddenly loses two control-plane nodes because of a hardware crash.
Only one control-plane node is alive.

Symptoms:

kubectl commands are failing or very slow

etcd logs show "lost quorum"

Cluster is technically running but cannot schedule or update anything

‚ùì Questions:

What does ‚Äúetcd lost quorum‚Äù mean?

Why does the cluster stop functioning even when one control-plane node is alive?

How do you restore quorum?

Using etcd snapshots

Or manually removing failed members

What commands do you run to recover etcd on the surviving node?


Answer ‚úÖ Full detailed answer (study / interview-grade)
A. Concepts you must state first (short)

etcd is the single source of truth for Kubernetes cluster state (pods, secrets, configmaps, node objects, etc.).

Quorum (majority of members) is required for read/write consistency. For 3-member cluster, quorum = 2.

Never run etcdctl snapshot restore on a running etcd node without stopping it ‚Äî that can corrupt data.

Always secure etcd communications (TLS certs), and use etcdctl v3 (ETCDCTL_API=3).

B. How to take a consistent etcd snapshot (recommended production steps)

Use etcdctl v3 and TLS. On a control-plane node:
Etcd backup & restore ‚Äî short actionable steps
Save snapshot (on a healthy control-plane node):
etcdctl --endpoints=$ENDPOINTS \
  --cacert=$ETCD_CERT_DIR/ca.crt \
  --cert=$ETCD_CERT_DIR/healthcheck-client.crt \
  --key=$ETCD_CERT_DIR/healthcheck-client.key \
  snapshot save /var/backups/etcd-snap-$(date -u +%Y%m%dT%H%M%SZ).db
Copy snapshot + /etc/kubernetes/pki to off-node durable storage (S3, NFS, etc).

B. Automate & validate snapshots

Cron or systemd timer to run snapshot save and upload to object storage.

Keep retention policy (daily x7, weekly x4).

Validate after each snapshot: etcdctl snapshot status and run a periodic test restore in CI (restore to ephemeral VM, start etcd, run kubectl get pods --all-namespaces).

Also backup kube-apiserver and kube-controller-manager certs and manifests (/etc/kubernetes).

C. Restore an etcd snapshot to recover a failed control-plane node (embedded etcd)
(Goal: restore the local data dir of the failed member; assume cluster still has quorum or you are rebuilding a single lost member)

On the failed control-plane node (or new replacement node): stop control-plane static pods / kubelet:

Copy snapshot and certs to the node.

Restore snapshot into a new data-dir:

ETCD_DATA_DIR=/var/lib/etcd
etcdctl snapshot restore /path/to/snap.db \
  --data-dir $ETCD_DATA_DIR \
  --name <member-name> \
  --initial-cluster "<m1>=https://IP1:2380,<m2>=https://IP2:2380,..." \
  --initial-cluster-token <token> \
  --initial-advertise-peer-urls https://<this-ip>:2380
sudo systemctl start kubelet
Verify the node is restored ..
